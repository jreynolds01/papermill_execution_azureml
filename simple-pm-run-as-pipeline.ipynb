{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "The goal of this notebook is to show how running a notebook via papermill as a pipeline differs from using `Experiment.submit()`. To see the examples for `Experiment.submit()`, please see this [notebook](simple-pm-run.ipynb). It assumes [simple-pm-run.ipynb](simple-pm-run.ipynb) has already been run to create resources.\n",
    "\n",
    "This notebook started as a copy of the Pipelines Getting Started Notebook [here](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-getting-started.ipynb).\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "This notebook requires that `azureml-sdk` is installed in the environment in which it is run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Machine Learning Imports\n",
    "\n",
    "In this first code cell, we import key Azure Machine Learning modules that we will use below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.15\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core import Workspace, Experiment\n",
    "from azureml.core.runconfig import CondaDependencies, RunConfiguration\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "\n",
    "## load pipeline dependencies\n",
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Workspace\n",
    "\n",
    "Initialize a [workspace](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.workspace(class%29) object from persisted configuration, or get it from Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aml_compute_target = \"aml-compute-d2\" ## 2-16 characters\n",
    "exp_name = 'papermill-in-a-pipeline'\n",
    "# project folder\n",
    "project_folder = './projectDir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Workspace information from configuration\n",
      "Found the config file in: C:\\Users\\jeremr\\Documents\\GitHub\\papermill_execution_azureml\\aml_config\\config.json\n",
      "jeremr_top10_mvl_aml\n",
      "jeremr_top10_mvl\n",
      "eastus\n",
      "03909a66-bef8-4d52-8e9a-a346604e0902\n"
     ]
    }
   ],
   "source": [
    "if os.path.isdir('aml_config'):\n",
    "    print('Loading Workspace information from configuration')\n",
    "    ws = Workspace.from_config()\n",
    "else:\n",
    "    print('Getting Workspace information from Variables. You must set these or this will fail!')\n",
    "    SUBSCRIPTION_ID = os.getenv(\"AZ_SUB\",\"\")\n",
    "    RESOURCE_GROUP = os.getenv(\"RESOURCE_GROUP\",\"\")\n",
    "    WS_NAME = os.getenv(\"WS_NAME\",\"\")\n",
    "    WS_LOCATION = 'eastus'\n",
    "    ws=Workspace.get(name=WS_NAME,\n",
    "                    resource_group=RESOURCE_GROUP,\n",
    "                    subscription_id=SUBSCRIPTION_ID)\n",
    "\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Targets\n",
    "\n",
    "We will pick up from the prior notebook and focus on cloud computing, and in this case, we'll continue to use AmlCompute for executing our pipeline step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List of Compute Targets on the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jeremr-top10-adb\n",
      "jeremr-top10-mvl\n",
      "top10-mvl-d4v2\n",
      "aml-compute-d2\n"
     ]
    }
   ],
   "source": [
    "cts = ws.compute_targets\n",
    "for ct in cts:\n",
    "    print(ct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## run_config.load() does not seem to work:\n",
    "# my_aml_run_config = RunConfiguration()\n",
    "# my_aml_run_config.load(path='.', name=aml_compute_target)\n",
    "# print(my_aml_run_config.target) ## still prints 'local'\n",
    "# it does not load the values...\n",
    "\n",
    "## so recreate\n",
    "aml_compute = AmlCompute(ws, aml_compute_target)\n",
    "\n",
    "cd = CondaDependencies.create(pip_packages=[\"ipykernel\", \"papermill\", \"azureml-sdk\"])\n",
    "my_aml_run_config = RunConfiguration(conda_dependencies=cd)\n",
    "my_aml_run_config.target = aml_compute_target\n",
    "my_aml_run_config.environment.docker.enabled = True\n",
    "my_aml_run_config.environment.docker.base_image = DEFAULT_CPU_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run as a Pipeline ...\n",
    "\n",
    "You would create a `PythonScriptStep`, and then a pipeline, and then submit the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 created\n"
     ]
    }
   ],
   "source": [
    "# Uses default values for PythonScriptStep construct.\n",
    "\n",
    "step1nohash = PythonScriptStep(name=\"Use papermill to run a notebook\",\n",
    "                         script_name=\"papermill_run_notebook.py\", \n",
    "                         compute_target=aml_compute, \n",
    "                         source_directory=project_folder,\n",
    "                         runconfig=my_aml_run_config,\n",
    "                         allow_reuse=False\n",
    "                        )\n",
    "print(\"Step1 created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build, Validate, and Submit the pipeline\n",
    "You have the option to [validate](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py#validate) the pipeline prior to submitting for run. The platform runs validation steps such as checking for circular dependencies and parameter checks etc. even if you do not explicitly call validate method.\n",
    "\n",
    "### Submit the pipeline\n",
    "[Submitting](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py#submit) the pipeline involves creating an [Experiment](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.experiment?view=azure-ml-py) object and providing the built pipeline for submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built\n",
      "Pipeline validation complete\n",
      "Created step Use papermill to run a notebook [cd15b7bd][cfd165e6-8440-4062-8a76-9f5a9925338c], (This step will run and generate new outputs)\n",
      "Submitted pipeline run: e9d66309-0ccc-4565-bdb6-62fa0e2c4137\n",
      "Pipeline is submitted for execution\n"
     ]
    }
   ],
   "source": [
    "pipeline1 = Pipeline(workspace=ws, steps=[step1nohash])\n",
    "print (\"Pipeline is built\")\n",
    "pipeline1.validate()\n",
    "print(\"Pipeline validation complete\")\n",
    "pipeline_run1 = Experiment(ws, exp_name).submit(pipeline1)\n",
    "print(\"Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for file changes\n",
    "\n",
    "Pipelines are different than experiments. Try editing `msg` in `hello_world.ipynb`, and then building and running the same pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 created\n",
      "Pipeline is built\n",
      "Step Use papermill to run a notebook is ready to be created [7f50341b]\n",
      "Pipeline validation complete\n",
      "Created step Use papermill to run a notebook [7f50341b][c4b665d4-b695-4805-8c5d-bc1fa52b4230], (This step will run and generate new outputs)\n",
      "Submitted pipeline run: ac016077-28b5-44e3-aaf7-745278243281\n",
      "Pipeline is submitted for execution\n"
     ]
    }
   ],
   "source": [
    "## Edit `msg` first!!\n",
    "step1nohash = PythonScriptStep(name=\"Use papermill to run a notebook\",\n",
    "                         script_name=\"papermill_run_notebook.py\", \n",
    "                         compute_target=aml_compute, \n",
    "                         source_directory=project_folder,\n",
    "                         runconfig=my_aml_run_config,\n",
    "                         allow_reuse=False\n",
    "                        )\n",
    "print(\"Step1 created\")\n",
    "pipeline1 = Pipeline(workspace=ws, steps=[step1nohash])\n",
    "print (\"Pipeline is built\")\n",
    "pipeline1.validate()\n",
    "print(\"Pipeline validation complete\")\n",
    "pipeline_run1 = Experiment(ws, exp_name).submit(pipeline1)\n",
    "print(\"Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Experiments\n",
    "\n",
    "If you tried various configurations of updating that file, the upshot is that by default, you only get updates from the notebook if you also update the script. Updating the script triggers an update of the entire source_dir. It doesn't matter if you rebuild the pipeline, or even rebuild the step - if you only update the notebook, you won't see new results.\n",
    "\n",
    "\n",
    "\n",
    "| run num |  pipeline updates              | msg status (nb)  | msg2 status (script) | msg log    | msg2 log |\n",
    "|---------|--------------------------------|-------------------|----------------------|------------|----------|\n",
    "| run 1   |  clean                         | hello world           | run1 | hello world     | run1      |\n",
    "| run 2   | **only tried Experiment.submit()** | hello world           | **run2** | hello world     | run1  |\n",
    "| run 3   | **built, validated, and submited** | hello world           | run2  | hello world     | **run2** |\n",
    "| run 4   | built, validated, and submited | hello world           | **run4**  | hello world     | **run4**  |\n",
    "| run 5   | built, validated, and submited | **'tst 2 - hello world'** | run4  | 'hello world'   | run4  |\n",
    "| run 6   | built, validated, and submited | **'change - hello world'**  | **run7** | **'change - hello world'**  | **run7** |\n",
    "| run 7   | built, validated, and submited | **'chng 2 - hello world'**  | run7 | 'change - hello world'  | run7 |\n",
    "| run 8   | built, validated, and submited | **'hello world'**  | run7 | 'change - hello world'  | run7 |\n",
    "| run 9   | built, validated, and submited | 'hello world'      | **run9** | **'hello world'**  | **run9** |\n",
    "| run 10  | **step def., built, validated, and submited** | 'hello world'      | **run10** | 'hello world'  | **run10** |\n",
    "| run 11  | step def., built, validated, and submited | **'delta: hello world'**      | run10 | 'hello world'  | run10 |\n",
    "| run 12  | step def., built, validated, and submited | **'no show: hello world'**      | run10 | 'hello world'  | run10 |\n",
    "| run 13  | step def., built, validated, and submited | 'no show: hello world'      | **run13** | **'no show: hello world'**  | **run13** |\n",
    "\n",
    "In order to change this behavior, you can do a few things.\n",
    "\n",
    "- Use the `hash_paths` argument to make sure that outputs are regenerated if key files (like the notebook) are checked for changes and resubmit if they are.\n",
    "- set `regenerate_outputs=True` when you submit.\n",
    "\n",
    "To see these two things in action, see the following sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using hash_paths\n",
    "\n",
    "Reset the hello_world notebook, and rebuild the pipeline by marking that file as a `hash_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 created\n",
      "Pipeline is built\n",
      "Step Use papermill to run a notebook is ready to be created [6bd9084b]\n",
      "Pipeline validation complete\n",
      "Created step Use papermill to run a notebook [6bd9084b][39f4878c-edb2-4abf-a70c-26b04c2ffa50], (This step will run and generate new outputs)\n",
      "Submitted pipeline run: fdd43239-ba45-4897-bdea-317288ccbb25\n",
      "Pipeline is submitted for execution\n"
     ]
    }
   ],
   "source": [
    "# Uses default values for PythonScriptStep construct.\n",
    "step1withhash = PythonScriptStep(name=\"Use papermill to run a notebook\",\n",
    "                         script_name=\"papermill_run_notebook.py\", \n",
    "                         compute_target=aml_compute, \n",
    "                         source_directory=project_folder,\n",
    "                         runconfig=my_aml_run_config,\n",
    "                         allow_reuse=False,\n",
    "                         hash_paths=['hello_world.ipynb']\n",
    "                        )\n",
    "print(\"Step1 created\")\n",
    "pipeline1 = Pipeline(workspace=ws, steps=[step1withhash])\n",
    "print (\"Pipeline is built\")\n",
    "pipeline1.validate()\n",
    "print(\"Pipeline validation complete\")\n",
    "pipeline_run1 = Experiment(ws, exp_name).submit(pipeline1)\n",
    "print(\"Pipeline is submitted for execution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built\n",
      "Pipeline validation complete\n",
      "Created step Use papermill to run a notebook [2aaf0bb9][8cde8f88-c183-4737-aac4-44429b9ecea7], (This step will run and generate new outputs)\n",
      "Submitted pipeline run: f20903f1-f0e1-41df-9f0a-0752121b8c3f\n",
      "Pipeline is submitted for execution\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pipeline1 = Pipeline(workspace=ws, steps=[step1withhash])\n",
    "print (\"Pipeline is built\")\n",
    "pipeline1.validate()\n",
    "print(\"Pipeline validation complete\")\n",
    "pipeline_run1 = Experiment(ws, exp_name).submit(pipeline1)\n",
    "print(\"Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Experiments with hash_paths\n",
    "\n",
    "The upshot is that if you specify hash_paths, then you will get a new value when that file changes. It works as intended.\n",
    "\n",
    "\n",
    "| run num |  pipeline updates              | msg status (nb)  | msg2 status (script) | msg log    | msg2 log |\n",
    "|---------|--------------------------------|-------------------|----------------------|------------|----------|\n",
    "| run 1   |  clean                         | hello world           | run1 | hello world     | run1      |\n",
    "| run 2   | **Experiment.submit()**        | hello world           | **run2** | hello world     | run1  |\n",
    "| run 3   | **built, validated, and submited** | hello world           | run2  | hello world     | **run2** |\n",
    "| run 4   | built, validated, and submited | hello world           | **run4**  | hello world     | **run4**  |\n",
    "| run 5   | built, validated, and submited | **'chng: Hello World!'** | run4  | **'chng: Hello World!'**  | run4  |\n",
    "| run 6   | built, validated, and submited | **'Hello World again!'** | run4  | **'Hello World again!'**  | run4  |\n",
    "| run 7   | **step def., built, validated, and submited** | 'Hello World again!'   | run4 | 'Hello World again!'  | run4 |\n",
    "| run 8    | step def., built, validated, and submited | **'Hello World!'**      | run4 | **'Hello World!'**  | run4 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore regenerate_outputs\n",
    "\n",
    "We can try the same experiment (with no hashing) to see if regenerate_outputs has an effect. In this case, because we know script changes trigger an update, we'll just manipulate the notebook, the pipeline upates, and whether regenerate_outputs is true or false on the submit call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step1 created\n",
      "Pipeline is built\n",
      "Pipeline validation complete\n",
      "Created step Use papermill to run a notebook [787419c2][cfd165e6-8440-4062-8a76-9f5a9925338c], (This step will run and generate new outputs)\n",
      "Submitted pipeline run: bf1ce206-8e2d-41f8-a3f4-20f7b0695045\n",
      "Pipeline is submitted for execution\n"
     ]
    }
   ],
   "source": [
    "## Edit `msg` first!!\n",
    "step1nohash = PythonScriptStep(name=\"Use papermill to run a notebook\",\n",
    "                         script_name=\"papermill_run_notebook.py\", \n",
    "                         compute_target=aml_compute, \n",
    "                         source_directory=project_folder,\n",
    "                         runconfig=my_aml_run_config,\n",
    "                         allow_reuse=False\n",
    "                        )\n",
    "print(\"Step1 created\")\n",
    "pipeline1 = Pipeline(workspace=ws, steps=[step1nohash])\n",
    "print (\"Pipeline is built\")\n",
    "pipeline1.validate()\n",
    "print(\"Pipeline validation complete\")\n",
    "pipeline_run1 = Experiment(ws, exp_name).submit(pipeline1, regenerate_outputs=False)\n",
    "print(\"Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built\n",
      "Pipeline validation complete\n",
      "Created step Use papermill to run a notebook [15ef3e31][bcd166f1-0f5c-4f13-a473-6c277a396cee], (This step will run and generate new outputs)\n",
      "Submitted pipeline run: f7916073-d1a5-4fdc-b8eb-1cb9c9009360\n",
      "Pipeline is submitted for execution\n"
     ]
    }
   ],
   "source": [
    "pipeline1 = Pipeline(workspace=ws, steps=[step1nohash])\n",
    "print (\"Pipeline is built\")\n",
    "pipeline1.validate()\n",
    "print(\"Pipeline validation complete\")\n",
    "pipeline_run1 = Experiment(ws, exp_name).submit(pipeline1, regenerate_outputs=True)\n",
    "print(\"Pipeline is submitted for execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results of Experiments with regenerate_outputs\n",
    "\n",
    "In all cases, `hash_paths=None`.\n",
    "\n",
    "`regenerate_outputs` basically enforces a new build, it would appear. The default value appears to be analogous to `regenerate_outputs=False`\n",
    "\n",
    "\n",
    "| run num |  regenerate_outputs | pipeline updates | msg status (nb)  | msg2 status (script) | msg log    | msg2 log |\n",
    "|---------|---------------------|-----------|-------------------|----------------------|------------|----------|\n",
    "| run 1   | 0 | clean                         | hello world           | run1 | hello world     | run1      |\n",
    "| run 2   | 0 | **Experiment.submit()**        | **tst2**          | run1 | hello world     | run1  |\n",
    "| run 3   | **1** | Experiment.submit()        | tst2          | run1 | hello world     | run1  |\n",
    "| run 4   | 1 | Experiment.submit()        | tst2          | **run4** | hello world     | run1  |\n",
    "| run 5   | 1 | **built, validated, and submited** | tst2           | **run1**  | **tst2**     | run1 |\n",
    "| run 6   | 1 | built, validated, and submited | **change**           | run1  | **change**     | run1 |\n",
    "| run 7   | 1 | built, validated, and submited | change           | run1  | change     | run1 |\n",
    "| run 8   | 1 | built, validated, and submited | **delta**           | run1  | **delta**     | run1 |\n",
    "| run 9   | 1 | built, validated, and submited | **gamma**           | run1  | **gamma**     | run1 |\n",
    "| run 10   | **0** | built, validated, and submited | **epsilon**           | run1  | gamma     | run1 |\n",
    "| run 11   | 0 | built, validated, and submited | **alpha**           | run1  | gamma     | run1 |\n",
    "| run 12   | **1** | built, validated, and submited | alpha           | run1  | **alpha**   | run1 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline is built\n",
      "Step Use papermill to run a notebook is ready to be created [13443c3c]\n",
      "Pipeline validation complete\n",
      "Created step Use papermill to run a notebook [13443c3c][653e657b-dce6-4f99-b43e-5107c295b821], (This step will run and generate new outputs)\n",
      "Submitted pipeline run: aea00d1e-732f-423a-acae-1c8c9257a3ef\n",
      "Pipeline is submitted for execution\n"
     ]
    }
   ],
   "source": [
    "## Edit `msg` first!!\n",
    "pipeline1 = Pipeline(workspace=ws, steps=[step1])\n",
    "print (\"Pipeline is built\")\n",
    "pipeline1.validate()\n",
    "print(\"Pipeline validation complete\")\n",
    "pipeline_run1 = Experiment(ws, exp_name).submit(pipeline1, regenerate_outputs=False)\n",
    "print(\"Pipeline is submitted for execution\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If regenerate_outputs is set to True, a new submit will always force generation of all step outputs, and disallow data reuse for any step of this run. Once this run is complete, however, subsequent runs may reuse the results of this run.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the pipeline run\n",
    "\n",
    "#### Use RunDetails Widget\n",
    "We are going to use the RunDetails widget to examine the run of the pipeline. You can click each row below to get more details on the step runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script: Use papermill to run a notebook status: Finished\n"
     ]
    },
    {
     "ename": "ErrorResponseException",
     "evalue": "(BadRequest) Response status code does not indicate success: 404 (Not Found).\nCould not find node status for node [f7916073-d1a5-4fdc-b8eb-1cb9c9009360/5746942a-0be2-453c-95ee-d2b2d6fcbdc4]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mErrorResponseException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-114-922b1795badd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# Change this if you want to see details even if the Step has succeeded.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mjoblog\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_job_log\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'job log:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoblog\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\pm_simple\\lib\\site-packages\\azureml\\pipeline\\core\\run.py\u001b[0m in \u001b[0;36mget_job_log\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    410\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \"\"\"\n\u001b[1;32m--> 412\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_step_run_provider\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_job_log\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pipeline_run_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_node_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_stdout_log\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\pm_simple\\lib\\site-packages\\azureml\\pipeline\\core\\_aeva_provider.py\u001b[0m in \u001b[0;36mget_job_log\u001b[1;34m(self, pipeline_run_id, node_id)\u001b[0m\n\u001b[0;32m    867\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         \"\"\"\n\u001b[1;32m--> 869\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_service_caller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_node_job_log_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline_run_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_stdout_log\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline_run_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\pm_simple\\lib\\site-packages\\azureml\\pipeline\\core\\_restclients\\aeva\\service_caller.py\u001b[0m in \u001b[0;36mget_node_job_log_async\u001b[1;34m(self, pipeline_run_id, node_id)\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[0msubscription_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_subscription_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_group_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_resource_group_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[0mworkspace_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_workspace_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline_run_id\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpipeline_run_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_id_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m             custom_headers=self._get_custom_headers())\n\u001b[0m\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\pm_simple\\lib\\site-packages\\azureml\\pipeline\\core\\_restclients\\aeva\\aml_pipelines_api10.py\u001b[0m in \u001b[0;36mapi_v10_subscriptions_by_subscription_id_resource_groups_by_resource_group_name_providers_microsoft_machine_learning_services_workspaces_by_workspace_name_pipeline_runs_by_pipeline_run_id_graph_shareable_job_log_post\u001b[1;34m(self, subscription_id, resource_group_name, workspace_name, pipeline_run_id, node_id_path, custom_headers, raw, **operation_config)\u001b[0m\n\u001b[0;32m   2630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2631\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2632\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mErrorResponseException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_deserialize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2634\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mErrorResponseException\u001b[0m: (BadRequest) Response status code does not indicate success: 404 (Not Found).\nCould not find node status for node [f7916073-d1a5-4fdc-b8eb-1cb9c9009360/5746942a-0be2-453c-95ee-d2b2d6fcbdc4]"
     ]
    }
   ],
   "source": [
    "step_runs = pipeline_run1.get_children()\n",
    "for step_run in step_runs:\n",
    "    status = step_run.get_status()\n",
    "    print('Script:', step_run.name, 'status:', status)\n",
    "    \n",
    "    # Change this if you want to see details even if the Step has succeeded.\n",
    "    joblog = step_run.get_job_log()\n",
    "    print('job log:', joblog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step_run"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "diray"
   }
  ],
  "kernelspec": {
   "display_name": "Python [conda env:pm_simple]",
   "language": "python",
   "name": "conda-env-pm_simple-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
